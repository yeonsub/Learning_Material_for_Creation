{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8w3y++8mEhiziEyRQpti6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeonsub/models_from_scratch/blob/main/Token_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt' tokenizer models (required for sent_tokenize and word_tokenize)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Download the 'punkt_tab' resource (required for word_tokenize)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7HcGL897n4j",
        "outputId": "3354b512-8285-4b72-d6a7-aea977308ca1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLTK is a powerful library for natural language processing in Python. It works well in Google Colab!\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "print(words)\n",
        "\n",
        "text = \"NLTK이 한글도 잘 처리하여 주면 좋겠는데\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "print(words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzak9CAh8Tll",
        "outputId": "873297bc-d89b-4a1c-cc6f-1022c6bb2106"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'in', 'Python', '.', 'It', 'works', 'well', 'in', 'Google', 'Colab', '!']\n",
            "['NLTK이', '한글도', '잘', '처리하여', '주면', '좋겠는데']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Apple's name was inspired by Steve Jobs' visit to an apple farm. His visit was while on a fruitarian diet.\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkY8eSH8_W8",
        "outputId": "376a7285-6599-4f75-d763-0c7893bd268e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Apple's name was inspired by Steve Jobs' visit to an apple farm.\n",
            "Sentence 2: His visit was while on a fruitarian diet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정규화된 문장만 처리"
      ],
      "metadata": {
        "id": "eQlqKTZV-BP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentence = \"Isn't this great? I can't tell.\"\n",
        "\n",
        "# Tokenizer that keeps only alphabetic characters and apostrophes/hyphens as tokens\n",
        "tokenizer = RegexpTokenizer(r\"[a-zA-Z'-]+\")\n",
        "\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmaqgpr399up",
        "outputId": "d3184132-229e-4b20-a8d1-78f1d9531074"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Isn't\", 'this', 'great', 'I', \"can't\", 'tell']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Gensim\n",
        "!pip install gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load a smaller pre-trained model for this example (e.g., 'glove-wiki-gigaword-50')\n",
        "# The full Google News model is ~3GB\n",
        "print(\"Downloading pre-trained GloVe model...\")\n",
        "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
        "print(\"Download complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzMWJbxSDpCb",
        "outputId": "0d724a0c-1870-4ffc-f981-a411291120de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Downloading pre-trained GloVe model...\n",
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector (embedding) for a specific word\n",
        "word = \"서울\"\n",
        "if word in glove_model:\n",
        "    word_vector = glove_model[word]\n",
        "    print(f\"\\nEmbedding for '{word}':\", word_vector)\n",
        "    print(f\"Embedding dimension for '{word}':\", len(word_vector))\n",
        "else:\n",
        "    print(f\"Word '{word}' not found in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNsm6urT-KBe",
        "outputId": "3d4c36b2-0028-424a-9276-b55de1ed6e9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word '서울' not found in vocabulary.\n"
          ]
        }
      ]
    }
  ]
}